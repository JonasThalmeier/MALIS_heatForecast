{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59dbbb72",
   "metadata": {},
   "source": [
    "## Initial Steps for Working with GRIB Dataset in Python\n",
    "\n",
    "### Step 1: Install Required Libraries\n",
    "You will need pygrib, xarray, numpy, pandas, and matplotlib.\n",
    "Use the following command to install them:\n",
    "!pip install pygrib xarray numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa56588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b6e25",
   "metadata": {},
   "source": [
    "### Step 2: Load and Explore the GRIB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6b88b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the GRIB dataset\n",
    "data = xr.open_dataset('./spain_may2sept.grib', engine=\"cfgrib\")\n",
    "\n",
    "# Print available coordinates\n",
    "print(\"Coordinates of the dataset:\")\n",
    "print(data.coords)\n",
    "\n",
    "# Optionally, print specific coordinate values\n",
    "print(\"\\nLatitude values:\")\n",
    "print(data[\"latitude\"].values)\n",
    "\n",
    "print(\"\\nLongitude values:\")\n",
    "print(data[\"longitude\"].values)\n",
    "\n",
    "# If the dataset has time:\n",
    "if \"time\" in data.coords:\n",
    "    print(\"\\nTime values:\")\n",
    "    print(data[\"time\"].values)\n",
    "    \n",
    "print(data.data_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a7ef5",
   "metadata": {},
   "source": [
    "## Generate labels for hot days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe8173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_region = data.sel(\n",
    "    latitude=slice(43, 38),  # From 43°N to 38°N (descending order)\n",
    "    longitude=slice(-8, -1)  # From 8°W to 1°W (ascending order)\n",
    ")\n",
    "# Step 1: Restrict the dataset to May–August\n",
    "data_summer = data_region.sel(time=data[\"time\"].dt.month.isin([5, 6, 7, 8, 9]))\n",
    "\n",
    "# Step 2: Extract temperature data (e.g., variable \"t2m\") for the summer months\n",
    "temperature_summer = data_summer[\"t2m\"]\n",
    "# Step 3: Filter temperature measurements at 12:00 within summer months\n",
    "# Step 3: Filter temperature measurements at 12:00 within summer months\n",
    "temp_summer_12 = temperature_summer.sel(time=temperature_summer[\"time\"].dt.hour == 12)\n",
    "\n",
    "# Convert the time to \"day\" in datetime format with time set to 00:00:00\n",
    "temp_summer_12 = temp_summer_12.assign_coords(day=temp_summer_12[\"time\"].dt.floor(\"D\"))\n",
    "\n",
    "# Use \"day\" as the main dimension\n",
    "temp_summer_12 = temp_summer_12.swap_dims({\"time\": \"day\"}).reset_coords(\"time\", drop=True)\n",
    "\n",
    "# Step 4: Extract July–August temperatures for percentile calculation\n",
    "temp_july_aug = temp_summer_12.sel(day=temp_summer_12[\"day\"].dt.month.isin([7, 8]))\n",
    "\n",
    "\n",
    "# Step 5: Compute the 95th percentile for each location in July–August\n",
    "percentile_95 = temp_july_aug.quantile(0.95, dim=\"day\")\n",
    "\n",
    "# Step 6: Label hot days (May–August) based on the 95th percentile\n",
    "hot_days = temp_summer_12 > percentile_95\n",
    "\n",
    "# Step 7: Add the \"hot_day\" label to the summer dataset\n",
    "data_summer_labeled = data_summer.assign(hot_day=hot_days)\n",
    "\n",
    "# Optional: Save the labeled dataset for further analysis\n",
    "# data_summer_labeled.to_netcdf(\"labeled_summer_dataset.nc\")\n",
    "\n",
    "# Step 8: Verify the results\n",
    "# print(\"Summer Dataset with Hot Day Labels:\")\n",
    "# print(data_summer_labeled.where(data_summer_labeled[\"hot_day\"], drop=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3db30ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out anomalies and hot days\n",
    "hot_days_aligned = hot_days.rename({\"day\": \"time\"})  # Align dimensions\n",
    "normal_days = ~hot_days_aligned  # Invert hot days to get normal days\n",
    "normal_data = data_summer.where(normal_days, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc488819",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hot days shape: {hot_days.shape}\")\n",
    "print(f\"Normal days shape: {normal_days.shape}\")\n",
    "print(f\"Total hot days: {hot_days.sum().item()}\")\n",
    "print(f\"Total normal days: {normal_days.sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd0f9e",
   "metadata": {},
   "source": [
    "## Aggregate all measurements of one day under one timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c250e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with merging and preparing features\n",
    "normal_data_00 = normal_data.sel(time=normal_data[\"time\"].dt.hour == 0)\n",
    "normal_data_12 = normal_data.sel(time=normal_data[\"time\"].dt.hour == 12)\n",
    "\n",
    "normal_data_00 = normal_data_00.assign_coords(day=normal_data_00[\"time\"].dt.floor(\"D\"))\n",
    "normal_data_12 = normal_data_12.assign_coords(day=normal_data_12[\"time\"].dt.floor(\"D\"))\n",
    "\n",
    "normal_data_00 = normal_data_00.swap_dims({\"time\": \"day\"}).reset_coords(\"time\", drop=True)\n",
    "normal_data_12 = normal_data_12.swap_dims({\"time\": \"day\"}).reset_coords(\"time\", drop=True)\n",
    "\n",
    "normal_data_00 = normal_data_00.rename({var: f\"{var}_00\" for var in normal_data_00.data_vars})\n",
    "normal_data_12 = normal_data_12.rename({var: f\"{var}_12\" for var in normal_data_12.data_vars})\n",
    "normal_data_00 = normal_data_00.drop_vars(\"valid_time\", errors=\"ignore\")\n",
    "normal_data_12 = normal_data_12.drop_vars(\"valid_time\", errors=\"ignore\")\n",
    "\n",
    "normal_data_merged = xr.merge([normal_data_00, normal_data_12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of hot days (True values) in the \"hot_day\" field\n",
    "num_hot_days = data_summer_labeled[\"hot_day\"].sum().item()\n",
    "\n",
    "print(f\"Total number of hot days: {num_hot_days}\")\n",
    "\n",
    "# Count the number of hot days (True values) in the \"hot_day\" field\n",
    "num_days = data_summer_labeled.dims[\"day\"]\n",
    "print(f\"Total number of days: {num_days}\")\n",
    "\n",
    "# Sum the count of non-NaN values in \"t2m\" across time, latitude, and longitude\n",
    "num_data_points = data_summer_labeled[\"t2m\"].notnull().sum().item()\n",
    "print(f\"Total number of data points: {num_data_points}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01354fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Temp Summer Dimensions:\", temp_summer_12.shape)\n",
    "print(\"Percentile 95 Shape:\", percentile_95.shape)\n",
    "print(\"Hot Days Shape:\", hot_days.shape)\n",
    "variable_names = list(data.data_vars)\n",
    "print(\"Variable names:\", variable_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371defc7",
   "metadata": {},
   "source": [
    "# Extract features and labels for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ed150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "processed_dir = \"temp_normal_features_by_year\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "years = np.unique(normal_data_merged[\"day\"].dt.year)\n",
    "feature_window_size = 30\n",
    "\n",
    "for year in years:\n",
    "    yearly_data = normal_data_merged.sel(day=(normal_data_merged[\"day\"].dt.year == year) &\n",
    "                                             normal_data_merged[\"day\"].dt.month.isin([5, 6, 7, 8]))\n",
    "\n",
    "    print(f\"Year: {year}, Data Shape: {yearly_data.sizes}\")\n",
    "\n",
    "    # Ensure no NaN values in yearly data\n",
    "    if yearly_data.isnull().any():\n",
    "        print(f\"Year {year}: Data contains NaN values. Filling NaNs with 0.\")\n",
    "        yearly_data = yearly_data.fillna(0)\n",
    "\n",
    "    rolling_chunk = (\n",
    "        yearly_data.rolling(day=feature_window_size, center=False)\n",
    "        .construct(\"feature_dim\")\n",
    "        .dropna(\"day\")\n",
    "    )\n",
    "\n",
    "    print(f\"Rolling chunk dimensions for year {year}: {rolling_chunk.dims}\")\n",
    "    print(f\"Rolling chunk sizes for year {year}: {rolling_chunk.sizes}\")\n",
    "\n",
    "    stacked_features = rolling_chunk.stack(location=(\"latitude\", \"longitude\"))\n",
    "    flattened_features = stacked_features.to_array(dim=\"variables\").stack(\n",
    "        features=(\"variables\", \"feature_dim\")).transpose(\"day\", \"location\", \"features\")\n",
    "\n",
    "    X = flattened_features.values.reshape(flattened_features.shape[0] * flattened_features.shape[1], -1)\n",
    "    \n",
    "    # Extract latitude, longitude, and percentile_95\n",
    "    latitudes = stacked_features[\"latitude\"].values\n",
    "    longitudes = stacked_features[\"longitude\"].values\n",
    "    percentile_95_values = percentile_95.stack(location=(\"latitude\", \"longitude\"))\n",
    "    repeated_percentile_95 = np.repeat(percentile_95_values, len(flattened_features[\"day\"]), axis=0)\n",
    "\n",
    "    repeated_latitudes = np.tile(latitudes, len(flattened_features[\"day\"]))\n",
    "    repeated_longitudes = np.tile(longitudes, len(flattened_features[\"day\"]))\n",
    "\n",
    "    lat_long_percentile = np.column_stack([repeated_latitudes, repeated_longitudes, repeated_percentile_95])\n",
    "    X = np.hstack((X, lat_long_percentile))\n",
    "\n",
    "    torch.save(torch.tensor(X, dtype=torch.float32), f\"{processed_dir}/features_{year}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b831cef",
   "metadata": {},
   "source": [
    "## Creating Target labels for each day (if the next 7 days contain at least 3 hot days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21e6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating labels for 7-day windows with at least 3 hot days\n",
    "hot_days_rolling = (\n",
    "    hot_days.rolling(day=7, center=False)\n",
    "    .construct(\"window_dim\")\n",
    "    .reduce(np.sum, dim=\"window_dim\")\n",
    ")\n",
    "labels_next_7_days = (hot_days_rolling >= 3).astype(int)\n",
    "labels_next_7_days = labels_next_7_days.shift(day=-6).dropna(\"day\")\n",
    "\n",
    "# Drop NaN values (caused by shifting)\n",
    "labels_next_7_days = labels_next_7_days.dropna(\"day\")\n",
    "\n",
    "# Debug: Inspect the rolling sum\n",
    "print(\"Rolling sum of hot days over 7-day window:\")\n",
    "print(hot_days_rolling)\n",
    "\n",
    "# Debug: Inspect the labels before and after shifting\n",
    "print(\"Labels before shifting (aligned with future interval):\")\n",
    "print((hot_days_rolling >= 3).astype(int))\n",
    "\n",
    "print(\"Labels after shifting (aligned with current day):\")\n",
    "print(labels_next_7_days)\n",
    "\n",
    "del hot_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861239fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import muppy, summary\n",
    "# Filter global variables for numpy arrays and xarray datasets\n",
    "arrays_and_datasets = {\n",
    "    name: type(value).__name__\n",
    "    for name, value in globals().items()\n",
    "    if type(value).__name__ in [\"ndarray\", \"Dataset\", \"DataArray\"]\n",
    "}\n",
    "\n",
    "# Print the results\n",
    "print(f\"{'Variable':<20}{'Type':<20}\")\n",
    "print(\"=\" * 40)\n",
    "for name, var_type in arrays_and_datasets.items():\n",
    "    print(f\"{name:<20}{var_type:<20}\")\n",
    "\n",
    "# Collect all objects in memory\n",
    "all_objects = muppy.get_objects()\n",
    "\n",
    "# Summarize memory usage by type\n",
    "summary.print_(summary.summarize(all_objects))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0125e",
   "metadata": {},
   "source": [
    "## Stacking data of 30 days onto each other and adding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8305e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_summer_merged.coords)\n",
    "#print(\"Rolling Chunk Days:\")\n",
    "#print(rolling_chunk[\"day\"].values)\n",
    "\n",
    "#print(\"\\nLabels Next 7 Days:\")\n",
    "#print(labels_next_7_days[\"day\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f228d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # Extract unique years from the 'day' coordinate\n",
    "# years = np.unique(data_summer_merged[\"day\"].dt.year)\n",
    "# feature_window_size = 30\n",
    "# percentile_95_stacked = percentile_95.stack(location=(\"latitude\", \"longitude\"))\n",
    "\n",
    "# # Directory for saving temporary results\n",
    "# processed_dir = \"temp_features_by_year\"\n",
    "# os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# for year in years:\n",
    "#     # Select data for the current year\n",
    "#     yearly_data = data_summer_merged.sel(\n",
    "#             day=(data_summer_merged[\"day\"].dt.year == year) &\n",
    "#                  (data_summer_merged[\"day\"].dt.month.isin([5, 6, 7, 8]))\n",
    "#     )\n",
    "#     # Create rolling windows\n",
    "#     rolling_chunk = (\n",
    "#         yearly_data.rolling(day=feature_window_size, center=False)\n",
    "#         .construct(\"feature_dim\")\n",
    "#         .dropna(\"day\")\n",
    "#     )\n",
    "    \n",
    "#     # Align features with labels\n",
    "#     yearly_labels = labels_next_7_days.sel(day=rolling_chunk[\"day\"])\n",
    "#     rolling_chunk, yearly_labels = xr.align(rolling_chunk, yearly_labels, join=\"inner\")\n",
    "\n",
    "    \n",
    "#     stacked_features = rolling_chunk.stack(location=(\"latitude\", \"longitude\"))\n",
    "#     flattened_features = stacked_features.to_array(dim=\"variables\").stack(features=(\"variables\", \"feature_dim\")).transpose(\"day\", \"location\", \"features\")\n",
    "#     X = flattened_features.values.reshape(flattened_features.shape[0] * flattened_features.shape[1], -1)\n",
    "\n",
    "#     # Extract latitude, longitude, and add percentile_95\n",
    "#     latitudes = stacked_features[\"latitude\"].values\n",
    "#     longitudes = stacked_features[\"longitude\"].values\n",
    "\n",
    "#     # Add `percentile_95` to each location\n",
    "#     percentile_95_values = percentile_95_stacked.sel(location=stacked_features[\"location\"]).values\n",
    "#     repeated_percentile_95 = np.repeat(percentile_95_values, len(flattened_features[\"day\"]), axis=0)\n",
    "\n",
    "#     # Repeat latitude and longitude for all records (days × locations)\n",
    "#     repeated_latitudes = np.tile(latitudes, len(flattened_features[\"day\"]))\n",
    "#     repeated_longitudes = np.tile(longitudes, len(flattened_features[\"day\"]))\n",
    "\n",
    "#     # Concatenate latitude, longitude, and percentile_95 to the features\n",
    "#     lat_long_percentile = np.column_stack([repeated_latitudes, repeated_longitudes, repeated_percentile_95])\n",
    "#     X = np.hstack((X, lat_long_percentile))  # Add these features to the rolling features\n",
    "\n",
    "\n",
    "#     # Check the resulting shape\n",
    "#     #stacked_labels = yearly_labels.stack(location=(\"latitude\", \"longitude\"))\n",
    "#     #flattened_labels = stacked_labels.to_array(dim=\"variables\").stack(features=(\"variables\", \"feature_dim\")).transpose(\"day\", \"location\", \"features\")\n",
    "#     #aligned_labels = flattened_labels.sel(day=stacked_features[\"day\"])\n",
    "#     #y = aligned_labels.values.flatten()  # Flatten into a single column\n",
    "    \n",
    "#     stacked_labels = yearly_labels.stack(location=(\"latitude\", \"longitude\"))\n",
    "#     y = stacked_labels.values.flatten()  # Flatten into a single column\n",
    "\n",
    "#     # Save to disk\n",
    "#     torch.save(torch.tensor(X, dtype=torch.float32), f\"{processed_dir}/features_{year}.pt\")\n",
    "#     torch.save(torch.tensor(y, dtype=torch.float32), f\"{processed_dir}/labels_{year}.pt\")\n",
    "    \n",
    "# print(\"Shape of flattened features (X_train):\", X.shape)\n",
    "# print(\"Shape of flattened labels (y_train):\", y.shape)\n",
    "\n",
    "# del data_summer_merged, hot_days_rolling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0b4af",
   "metadata": {},
   "source": [
    "## Train/Test split and flattening the arrays so they can be processed by NN\n",
    "The final array will be 2D. Each record corresponds with one location and one day. This record contains the data of the previous 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b29335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = \"temp_features_by_year\"\n",
    "\n",
    "# Load and train incrementally\n",
    "def load_data_for_year(year):\n",
    "    X = torch.load(f\"{processed_dir}/features_{year}.pt\")\n",
    "    y = torch.load(f\"{processed_dir}/labels_{year}.pt\")    \n",
    "    return X, y\n",
    "\n",
    "# Initialize DataLoader for incremental training\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data_for_year(2020)\n",
    "print(X_train.shape[1])\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fb12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "years = np.linspace(2006,2024,24-6+1).astype(int)\n",
    "train_years = years[years < 2020]\n",
    "test_years = years[years >= 2020]\n",
    "\n",
    "# Train Random Forest on the entire training set\n",
    "X_train, y_train = [], []  # Collect all training data for Random Forest\n",
    "\n",
    "for year in train_years:\n",
    "    X_train_year, y_train_year = load_data_for_year(year)\n",
    "    print(f\"Year {year}: Features shape: {X_train_year.shape}\")\n",
    "\n",
    "    X_train.append(X_train_year.numpy())  # Convert to numpy for sklearn\n",
    "    y_train.append(y_train_year.numpy())\n",
    "\n",
    "# Concatenate yearly data for Random Forest\n",
    "X_train = np.vstack(X_train)\n",
    "y_train = np.hstack(y_train)\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "\n",
    "# Compute global mean and standard deviation from the full training set\n",
    "global_mean = np.mean(X_train, axis=0)\n",
    "global_std = np.std(X_train, axis=0)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69116406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "lr_model = LogisticRegression(\n",
    "    penalty=\"l2\",  # Regularization\n",
    "    C=1.0,         # Regularization strength (1.0 is default)\n",
    "    solver=\"lbfgs\", # Solver for large datasets\n",
    "    #solver=\"saga\"\n",
    "    max_iter=500,  # Increase iterations for convergence\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3837287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest on the test set\n",
    "X_test, y_test = [], []\n",
    "\n",
    "for year in test_years:\n",
    "    X_val_year, y_val_year = load_data_for_year(year)\n",
    "    X_test.append(X_val_year.numpy())\n",
    "    y_test.append(y_val_year.numpy())\n",
    "\n",
    "# Concatenate test data\n",
    "X_test = np.vstack(X_test)\n",
    "y_test = np.hstack(y_test)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predictions\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest\n",
    "accuracy = accuracy_score(y_test, lr_predictions)\n",
    "precision = precision_score(y_test, lr_predictions)\n",
    "recall = recall_score(y_test, lr_predictions)\n",
    "f1 = f1_score(y_test, lr_predictions)\n",
    "\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93835d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features, mean, std):\n",
    "    \"\"\"\n",
    "    Normalize features using the given mean and standard deviation.\n",
    "\n",
    "    Args:\n",
    "        features (torch.Tensor or np.ndarray): The feature data to normalize.\n",
    "        mean (torch.Tensor or float): The global mean for each feature.\n",
    "        std (torch.Tensor or float): The global standard deviation for each feature.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized features.\n",
    "    \"\"\"\n",
    "    return (features - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=500, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96bcb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train_rf, return_counts=True)\n",
    "print(f\"Class distribution in training data: {dict(zip(unique, counts))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0170002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the plot\n",
    "fig, ax1 = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "# Plot training loss on the primary y-axis\n",
    "ax1.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", color=\"blue\", marker=\"o\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Training Loss\", color=\"blue\")\n",
    "ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "\n",
    "# Plot validation loss on the secondary y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", color=\"red\", marker=\"o\")\n",
    "ax2.set_ylabel(\"Validation Loss\", color=\"red\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"red\")\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Training and Validation Loss with Separate Y-Axes\")\n",
    "\n",
    "# Save the plot as a high-resolution PNG\n",
    "plt.savefig(\"training_validation_loss_Iteration4.png\", dpi=300, bbox_inches=\"tight\")  # 300 DPI for high resolution\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c0dab",
   "metadata": {},
   "source": [
    "### Using different NN and other models to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Evaluate on test data\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for year in test_years:\n",
    "    X_test, y_test = load_data_for_year(year)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Normalize features\n",
    "    X_test = (X_test - X_test.mean(dim=0)) / X_test.std(dim=0)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test).squeeze()\n",
    "        all_predictions.append((predictions > 0.5).float().numpy())\n",
    "        all_labels.append(y_test.numpy())\n",
    "\n",
    "# Combine results and evaluate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e17fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_rate = torch.sum(y_train == 1).item() / y_train.size(0)\n",
    "print(f\"Positive rate: {positive_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset and data loader\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class HotDayPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HotDayPredictor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "model = HotDayPredictor(input_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(30):\n",
    "    for batch in dataloader:\n",
    "        features, labels = batch\n",
    "        predictions = model(features)\n",
    "        loss = loss_fn(predictions.squeeze(), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming test_features and test_labels are prepared as PyTorch tensors\n",
    "X_test = flattened_features_test.values.reshape(flattened_features_test.shape[0] * flattened_features_test.shape[1], -1)\n",
    "y_test = aligned_labels_test.values.flatten()  # Flatten into a single column\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).squeeze()\n",
    "    predicted_classes = (predictions > 0.5).float()  # Convert probabilities to binary classes\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test.numpy(), predicted_classes.numpy())\n",
    "precision = precision_score(y_test.numpy(), predicted_classes.numpy())\n",
    "recall = recall_score(y_test.numpy(), predicted_classes.numpy())\n",
    "f1 = f1_score(y_test.numpy(), predicted_classes.numpy())\n",
    "print(\"Initial NN\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class DeeperHotDayPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DeeperHotDayPredictor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "model = DeeperHotDayPredictor(input_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(30):\n",
    "    for batch in dataloader:\n",
    "        features, labels = batch\n",
    "        predictions = model(features)\n",
    "        loss = loss_fn(predictions.squeeze(), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming test_features and test_labels are prepared as PyTorch tensors\n",
    "X_test = flattened_features_test.values.reshape(flattened_features_test.shape[0] * flattened_features_test.shape[1], -1)\n",
    "y_test = aligned_labels_test.values.flatten()  # Flatten into a single column\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).squeeze()\n",
    "    predicted_classes = (predictions > 0.5).float()  # Convert probabilities to binary classes\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test.numpy(), predicted_classes.numpy())\n",
    "precision = precision_score(y_test.numpy(), predicted_classes.numpy())\n",
    "recall = recall_score(y_test.numpy(), predicted_classes.numpy())\n",
    "f1 = f1_score(y_test.numpy(), predicted_classes.numpy())\n",
    "print(\"Deeper NN\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212894c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class DropoutHotDayPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DropoutHotDayPredictor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "model = DropoutHotDayPredictor(input_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(30):\n",
    "    for batch in dataloader:\n",
    "        features, labels = batch\n",
    "        predictions = model(features)\n",
    "        loss = loss_fn(predictions.squeeze(), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming test_features and test_labels are prepared as PyTorch tensors\n",
    "X_test = flattened_features_test.values.reshape(flattened_features_test.shape[0] * flattened_features_test.shape[1], -1)\n",
    "y_test = aligned_labels_test.values.flatten()  # Flatten into a single column\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).squeeze()\n",
    "    predicted_classes = (predictions > 0.5).float()  # Convert probabilities to binary classes\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test.numpy(), predicted_classes.numpy())\n",
    "precision = precision_score(y_test.numpy(), predicted_classes.numpy())\n",
    "recall = recall_score(y_test.numpy(), predicted_classes.numpy())\n",
    "f1 = f1_score(y_test.numpy(), predicted_classes.numpy())\n",
    "print(\"Dropout NN\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fccf9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Assuming test_features and test_labels are prepared as PyTorch tensors\n",
    "X_test = flattened_features_test.values.reshape(flattened_features_test.shape[0] * flattened_features_test.shape[1], -1)\n",
    "y_test = aligned_labels_test.values.flatten()  # Flatten into a single column\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_classes = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "precision = precision_score(y_test, predicted_classes)\n",
    "recall = recall_score(y_test, predicted_classes)\n",
    "f1 = f1_score(y_test, predicted_classes)\n",
    "\n",
    "print(\"Random Forest\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.data_vars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
