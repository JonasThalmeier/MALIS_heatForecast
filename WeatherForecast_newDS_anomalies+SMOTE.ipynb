{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hot Day Prediction Pipeline\n",
    "\n",
    "This notebook implements a machine learning pipeline for predicting hot days using temperature data from GRIB files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(grib_file_path, low_temp_threshold=285, high_temp_percentile=0.95, window_size=7):\n",
    "    \"\"\"\n",
    "    Load and preprocess GRIB dataset\n",
    "    \n",
    "    Args:\n",
    "        grib_file_path (str): Path to GRIB file\n",
    "        low_temp_threshold (float): Threshold for low temperature anomalies\n",
    "        high_temp_percentile (float): Percentile for defining hot days\n",
    "        window_size (int): Rolling window size for heatwave detection\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Processed temperature data and percentile\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    data = xr.open_dataset(grib_file_path, engine=\"cfgrib\")\n",
    "    \n",
    "    # Select region and time period\n",
    "    data_region = data.sel(\n",
    "        latitude=slice(43, 38),  # From 43째N to 38째N\n",
    "        longitude=slice(-8, -1)  # From 8째W to 1째W\n",
    "    )\n",
    "    data_summer = data_region.sel(time=data_region[\"time\"].dt.month.isin([5, 6, 7, 8, 9]))\n",
    "\n",
    "    # Extract temperature at noon\n",
    "    temperature_summer = data_summer[\"t2m\"]\n",
    "    temp_summer_12 = temperature_summer.sel(time=temperature_summer[\"time\"].dt.hour == 12)\n",
    "    temp_summer_12 = temp_summer_12.assign_coords(day=temp_summer_12[\"time\"].dt.floor(\"D\"))\n",
    "    temp_summer_12 = temp_summer_12.swap_dims({\"time\": \"day\"}).reset_coords(\"time\", drop=True)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    temp_data = temp_summer_12.to_dataframe().reset_index()\n",
    "    \n",
    "    # Filter out consistently low-temperature locations\n",
    "    mean_temp_by_location = temp_data.groupby([\"latitude\", \"longitude\"])[\"t2m\"].mean()\n",
    "    low_temp_locations = mean_temp_by_location[mean_temp_by_location <= low_temp_threshold].index\n",
    "    temp_data = temp_data[~temp_data.set_index([\"latitude\", \"longitude\"]).index.isin(low_temp_locations)].reset_index(drop=True)\n",
    "\n",
    "    # Calculate percentiles and hot day labels\n",
    "    percentile_95 = temp_data[\"t2m\"].quantile(high_temp_percentile)\n",
    "    \n",
    "    # Generate hot day labels\n",
    "    hot_days = temp_data[\"t2m\"] > percentile_95\n",
    "    hot_days_rolling = hot_days.rolling(window=window_size, center=False).sum()\n",
    "    labels_next_7_days = (hot_days_rolling >= 3).astype(int)\n",
    "    \n",
    "    # Align labels with the dataset\n",
    "    temp_data[\"heatwave_label\"] = labels_next_7_days.shift(-window_size + 1).fillna(0).astype(int)\n",
    "    \n",
    "    return temp_data, percentile_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_temperature_anomalies(temp_data):\n",
    "    \"\"\"\n",
    "    Detect temperature anomalies using Isolation Forest over the entire dataset.\n",
    "\n",
    "    Args:\n",
    "        temp_data (pd.DataFrame): Full temperature dataset.\n",
    "        contamination (float): Proportion of anomalies expected in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with anomaly flags.\n",
    "    \"\"\"\n",
    "    # Copy data to avoid modifying the original\n",
    "    temp_data_copy = temp_data.copy()\n",
    "\n",
    "    # Prepare data for Isolation Forest (only numerical features)\n",
    "    features = temp_data_copy[[\"t2m\"]]\n",
    "    \n",
    "    # Train Isolation Forest\n",
    "    anomaly_detector = IsolationForest(\n",
    "        contamination=0.01, random_state=42\n",
    "    )\n",
    "    anomaly_detector.fit(features)\n",
    "\n",
    "    # Predict anomalies\n",
    "    temp_data_copy[\"is_anomaly\"] = anomaly_detector.predict(features) == -1  # -1 indicates anomaly\n",
    "\n",
    "    return temp_data_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(data, feature_window_size=30, window_size=7):\n",
    "    \"\"\"\n",
    "    Prepare features for machine learning models\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Processed temperature data\n",
    "        feature_window_size (int): Size of feature rolling window\n",
    "        window_size (int): Size of heatwave prediction window\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Features and labels\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Filter out anomalous data\n",
    "    normal_data = data[~data[\"is_anomaly\"]]\n",
    "    \n",
    "    unique_locations = normal_data[[\"latitude\", \"longitude\"]].drop_duplicates()\n",
    "    \n",
    "    for _, location in unique_locations.iterrows():\n",
    "        location_data = normal_data[\n",
    "            (normal_data[\"latitude\"] == location[\"latitude\"]) & \n",
    "            (normal_data[\"longitude\"] == location[\"longitude\"])\n",
    "        ].sort_values(\"day\")\n",
    "        \n",
    "        # Percentile for this location from the entire dataset\n",
    "        location_percentile_95 = data[\"t2m\"].quantile(0.95)\n",
    "        \n",
    "        # Create rolling windows of features\n",
    "        for i in range(len(location_data) - feature_window_size - window_size + 1):\n",
    "            features = location_data.iloc[i:i+feature_window_size][\"t2m\"].values\n",
    "            label = location_data.iloc[i+feature_window_size+window_size-1][\"heatwave_label\"]\n",
    "            \n",
    "            X.append(np.concatenate([\n",
    "                features, \n",
    "                [location[\"latitude\"], location[\"longitude\"], location_percentile_95]\n",
    "            ]))\n",
    "            y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(input_size):\n",
    "    \"\"\"\n",
    "    Create a neural network model for hot day prediction\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Number of input features\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: Neural network model\n",
    "    \"\"\"\n",
    "    class HotDayPredictor(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(input_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.fc(x)\n",
    "    \n",
    "    return HotDayPredictor(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(model, X_train, y_train, epochs=30, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the neural network\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Neural network model\n",
    "        X_train (np.ndarray): Training features\n",
    "        y_train (np.ndarray): Training labels\n",
    "        epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: Trained neural network\n",
    "    \"\"\"\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_features, batch_labels in dataloader:\n",
    "            predictions = model(batch_features).squeeze()\n",
    "            loss = loss_fn(predictions, batch_labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train multiple machine learning models\n",
    "    \n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features\n",
    "        y_train (np.ndarray): Training labels\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Trained models and scaler\n",
    "    \"\"\"\n",
    "\n",
    "    # Balance training data with SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_balanced_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=500, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "        #'Neural Network': create_neural_network(X_train.shape[1])\n",
    "    }\n",
    "    \n",
    "    # Train models\n",
    "    for name, model in models.items():\n",
    "        if name == 'Neural Network':\n",
    "            # Neural network training\n",
    "            model = train_neural_network(model, X_train_balanced_scaled, y_train_balanced)\n",
    "        else:\n",
    "            # Sklearn models\n",
    "            model.fit(X_train_balanced_scaled, y_train_balanced)\n",
    "        \n",
    "        models[name] = model\n",
    "    \n",
    "    return models, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, scaler, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate trained models\n",
    "    \n",
    "    Args:\n",
    "        models (dict): Trained models\n",
    "        scaler (StandardScaler): Feature scaler\n",
    "        X_test (np.ndarray): Test features\n",
    "        y_test (np.ndarray): Test labels\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model performance metrics\n",
    "    \"\"\"\n",
    "    # Standardize test features\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Predictions\n",
    "        if name == 'Neural Network':\n",
    "            # For neural network\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "                predictions = model(X_test_tensor).squeeze().numpy()\n",
    "                predicted_classes = (predictions > 0.5).astype(int)\n",
    "        else:\n",
    "            # For sklearn models\n",
    "            predicted_classes = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results[name] = {\n",
    "            'accuracy': accuracy_score(y_test, predicted_classes),\n",
    "            'precision': precision_score(y_test, predicted_classes),\n",
    "            'recall': recall_score(y_test, predicted_classes),\n",
    "            'f1_score': f1_score(y_test, predicted_classes)\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hot_day_prediction_pipeline(grib_file_path, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Run the complete hot day prediction pipeline\n",
    "    \n",
    "    Args:\n",
    "        grib_file_path (str): Path to GRIB file\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Model results, trained models, and scaler\n",
    "    \"\"\"\n",
    "    # Preprocess data\n",
    "    temp_data, percentile_95 = load_and_preprocess_data(grib_file_path)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    temp_data_with_anomalies = detect_temperature_anomalies(temp_data)\n",
    "    \n",
    "    # Prepare features\n",
    "    X, y = prepare_features(temp_data_with_anomalies)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Train models\n",
    "    models, scaler = train_models(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    results = evaluate_models(models, scaler, X_test, y_test)\n",
    "    \n",
    "    return results, models, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping variable: paramId==235033 shortName='msshf'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ana\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 725, in build_dataset_components\n",
      "    dict_merge(variables, coord_vars)\n",
      "  File \"c:\\Users\\Ana\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 641, in dict_merge\n",
      "    raise DatasetBuildError(\n",
      "cfgrib.dataset.DatasetBuildError: key present and new value is different: key='time' value=Variable(dimensions=('time',), data=array([1143849600, 1143892800, 1143936000, ..., 1727611200, 1727654400,\n",
      "       1727697600])) new_value=Variable(dimensions=('time',), data=array([1143828000, 1143871200, 1143914400, ..., 1727589600, 1727632800,\n",
      "       1727676000]))\n",
      "skipping variable: paramId==147 shortName='slhf'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ana\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 725, in build_dataset_components\n",
      "    dict_merge(variables, coord_vars)\n",
      "  File \"c:\\Users\\Ana\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 641, in dict_merge\n",
      "    raise DatasetBuildError(\n",
      "cfgrib.dataset.DatasetBuildError: key present and new value is different: key='time' value=Variable(dimensions=('time',), data=array([1143849600, 1143892800, 1143936000, ..., 1727611200, 1727654400,\n",
      "       1727697600])) new_value=Variable(dimensions=('time',), data=array([1143828000, 1143871200, 1143914400, ..., 1727589600, 1727632800,\n",
      "       1727676000]))\n",
      "skipping variable: paramId==169 shortName='ssrd'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ana\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 725, in build_dataset_components\n",
      "    dict_merge(variables, coord_vars)\n",
      "  File \"c:\\Users\\Ana\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 641, in dict_merge\n",
      "    raise DatasetBuildError(\n",
      "cfgrib.dataset.DatasetBuildError: key present and new value is different: key='time' value=Variable(dimensions=('time',), data=array([1143849600, 1143892800, 1143936000, ..., 1727611200, 1727654400,\n",
      "       1727697600])) new_value=Variable(dimensions=('time',), data=array([1143828000, 1143871200, 1143914400, ..., 1727589600, 1727632800,\n",
      "       1727676000]))\n",
      "skipping variable: paramId==212 shortName='tisr'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ana\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 725, in build_dataset_components\n",
      "    dict_merge(variables, coord_vars)\n",
      "  File \"c:\\Users\\Ana\\anaconda3\\Lib\\site-packages\\cfgrib\\dataset.py\", line 641, in dict_merge\n",
      "    raise DatasetBuildError(\n",
      "cfgrib.dataset.DatasetBuildError: key present and new value is different: key='time' value=Variable(dimensions=('time',), data=array([1143849600, 1143892800, 1143936000, ..., 1727611200, 1727654400,\n",
      "       1727697600])) new_value=Variable(dimensions=('time',), data=array([1143828000, 1143871200, 1143914400, ..., 1727589600, 1727632800,\n",
      "       1727676000]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.7449\n",
      "Precision: 0.1586\n",
      "Recall: 0.8116\n",
      "F1_score: 0.2653\n",
      "\n",
      "Random Forest Performance:\n",
      "Accuracy: 0.8019\n",
      "Precision: 0.2089\n",
      "Recall: 0.8932\n",
      "F1_score: 0.3386\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "# Path to GRIB file\n",
    "grib_file_path = './spain_may2sept.grib'\n",
    "\n",
    "# Run pipeline\n",
    "results, models, scaler = run_hot_day_prediction_pipeline(grib_file_path)\n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f'\\n{model_name} Performance:')\n",
    "    for metric, value in metrics.items():\n",
    "        print(f'{metric.capitalize()}: {value:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
